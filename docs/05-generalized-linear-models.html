<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Angel Feliz">
<title>Machine Learning with R Notes - 5&nbsp; Generalized linear models (GLM)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-generative-classification-models.html" rel="next">
<link href="./04-resampling-methods.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="summary-format.css">
</head>
<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg navbar-dark "><div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Machine Learning with R Notes</span>
    </a>
  </div>
        <div class="quarto-navbar-tools ms-auto">
    <a href="https://github.com/AngelFelizR/ISL-Solutuon-Book" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./01-modeling-process.html">Summary Notes</a></li><li class="breadcrumb-item"><a href="./05-generalized-linear-models.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Generalized linear models (GLM)</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Summary Notes</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-modeling-process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Strategy to implement machine learning solutions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-feature-engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Dealing with missingness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-model-performance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Understanding model performance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-resampling-methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Resampling methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-generalized-linear-models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Generalized linear models (GLM)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-generative-classification-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generative Models for Classiﬁcation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-non-parametric-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Non-parametric Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-survival-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Time-event (Survival) Analysis and Censored Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-unsupervised-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">ISLR Execices 2nd Edition</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./execises-02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">02 - Statistical Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./execises-03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">03 - Linear Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./execises-04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">04 - Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./execises-05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">05 - Resampling Methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./execises-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">11 - Survival Analysis and Censored Data</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#linear-regresion" id="toc-linear-regresion" class="nav-link active" data-scroll-target="#linear-regresion"><span class="header-section-number">5.1</span> Linear regresion</a>
  <ul>
<li><a href="#getting-the-least-squares-line-of-a-sample" id="toc-getting-the-least-squares-line-of-a-sample" class="nav-link" data-scroll-target="#getting-the-least-squares-line-of-a-sample"><span class="header-section-number">5.1.1</span> Getting the Least Squares Line of a sample</a></li>
  <li><a href="#getting-confident-intervarls-of-coeffients" id="toc-getting-confident-intervarls-of-coeffients" class="nav-link" data-scroll-target="#getting-confident-intervarls-of-coeffients"><span class="header-section-number">5.1.2</span> Getting confident intervarls of coeffients</a></li>
  <li>
<a href="#insights-to-extract" id="toc-insights-to-extract" class="nav-link" data-scroll-target="#insights-to-extract"><span class="header-section-number">5.1.3</span> Insights to extract</a>
  <ul class="collapse">
<li><a href="#confirm-the-relationship-between-the-response-and-predictors" id="toc-confirm-the-relationship-between-the-response-and-predictors" class="nav-link" data-scroll-target="#confirm-the-relationship-between-the-response-and-predictors">Confirm the relationship between the Response and Predictors</a></li>
  <li><a href="#accuracy-of-the-model-relationship-strength" id="toc-accuracy-of-the-model-relationship-strength" class="nav-link" data-scroll-target="#accuracy-of-the-model-relationship-strength">Accuracy of the model (relationship strength)</a></li>
  <li><a href="#confirm-the-relationship-between-the-response-and-each-predictor" id="toc-confirm-the-relationship-between-the-response-and-each-predictor" class="nav-link" data-scroll-target="#confirm-the-relationship-between-the-response-and-each-predictor">Confirm the relationship between the Response and each predictor</a></li>
  <li><a href="#size-of-association-between-each-predictor-and-the-response." id="toc-size-of-association-between-each-predictor-and-the-response." class="nav-link" data-scroll-target="#size-of-association-between-each-predictor-and-the-response.">Size of association between each predictor and the response.</a></li>
  <li><a href="#predicting-future-values" id="toc-predicting-future-values" class="nav-link" data-scroll-target="#predicting-future-values">Predicting future values</a></li>
  </ul>
</li>
  <li>
<a href="#standard-linear-regression-model-assumptions" id="toc-standard-linear-regression-model-assumptions" class="nav-link" data-scroll-target="#standard-linear-regression-model-assumptions"><span class="header-section-number">5.1.4</span> Standard linear regression model assumptions</a>
  <ul class="collapse">
<li><a href="#including-an-interaction-term" id="toc-including-an-interaction-term" class="nav-link" data-scroll-target="#including-an-interaction-term">Including an interaction term</a></li>
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression">Polynomial regression</a></li>
  </ul>
</li>
  <li>
<a href="#possible-problems" id="toc-possible-problems" class="nav-link" data-scroll-target="#possible-problems"><span class="header-section-number">5.1.5</span> Possible problems</a>
  <ul class="collapse">
<li><a href="#the-target-doesnt-follow-a-normal-distribution" id="toc-the-target-doesnt-follow-a-normal-distribution" class="nav-link" data-scroll-target="#the-target-doesnt-follow-a-normal-distribution">The target doesn’t follow a normal distribution</a></li>
  <li><a href="#collinearity" id="toc-collinearity" class="nav-link" data-scroll-target="#collinearity">Collinearity</a></li>
  <li><a href="#non-linearity-of-the-response-predictor-relationships" id="toc-non-linearity-of-the-response-predictor-relationships" class="nav-link" data-scroll-target="#non-linearity-of-the-response-predictor-relationships">Non-linearity of the response-predictor relationships</a></li>
  <li><a href="#correlation-of-error-terms" id="toc-correlation-of-error-terms" class="nav-link" data-scroll-target="#correlation-of-error-terms">Correlation of error terms</a></li>
  <li><a href="#outliers" id="toc-outliers" class="nav-link" data-scroll-target="#outliers">Outliers</a></li>
  <li><a href="#high-leverage-points" id="toc-high-leverage-points" class="nav-link" data-scroll-target="#high-leverage-points">High-leverage points</a></li>
  </ul>
</li>
  <li><a href="#avoid-using-for-classification-problems" id="toc-avoid-using-for-classification-problems" class="nav-link" data-scroll-target="#avoid-using-for-classification-problems"><span class="header-section-number">5.1.6</span> Avoid using for classification problems</a></li>
  <li><a href="#benefits-of-replacing-plain-least-squares-%EF%AC%81tting" id="toc-benefits-of-replacing-plain-least-squares-ﬁtting" class="nav-link" data-scroll-target="#benefits-of-replacing-plain-least-squares-%EF%AC%81tting"><span class="header-section-number">5.1.7</span> Benefits of Replacing plain least squares ﬁtting</a></li>
  <li><a href="#coding-example" id="toc-coding-example" class="nav-link" data-scroll-target="#coding-example"><span class="header-section-number">5.1.8</span> Coding example</a></li>
  </ul>
</li>
  <li>
<a href="#extending-linear-regression-data-transformations" id="toc-extending-linear-regression-data-transformations" class="nav-link" data-scroll-target="#extending-linear-regression-data-transformations"><span class="header-section-number">5.2</span> Extending Linear Regression (Data Transformations)</a>
  <ul>
<li><a href="#polynomial-regression-1" id="toc-polynomial-regression-1" class="nav-link" data-scroll-target="#polynomial-regression-1"><span class="header-section-number">5.2.1</span> Polynomial regression</a></li>
  <li><a href="#piecewise-constant-regression" id="toc-piecewise-constant-regression" class="nav-link" data-scroll-target="#piecewise-constant-regression"><span class="header-section-number">5.2.2</span> Piecewise constant regression</a></li>
  <li><a href="#piecewise-polynomials-regression-natural-spine" id="toc-piecewise-polynomials-regression-natural-spine" class="nav-link" data-scroll-target="#piecewise-polynomials-regression-natural-spine"><span class="header-section-number">5.2.3</span> Piecewise polynomials regression (Natural Spine)</a></li>
  </ul>
</li>
  <li>
<a href="#subset-selection" id="toc-subset-selection" class="nav-link" data-scroll-target="#subset-selection"><span class="header-section-number">5.3</span> Subset Selection</a>
  <ul>
<li><a href="#best-subset-selection" id="toc-best-subset-selection" class="nav-link" data-scroll-target="#best-subset-selection"><span class="header-section-number">5.3.1</span> Best Subset Selection</a></li>
  <li><a href="#stepwise-selection-forward-stepwise-selection" id="toc-stepwise-selection-forward-stepwise-selection" class="nav-link" data-scroll-target="#stepwise-selection-forward-stepwise-selection"><span class="header-section-number">5.3.2</span> Stepwise Selection: Forward Stepwise Selection</a></li>
  <li><a href="#stepwise-selection-backward-stepwise-selection" id="toc-stepwise-selection-backward-stepwise-selection" class="nav-link" data-scroll-target="#stepwise-selection-backward-stepwise-selection"><span class="header-section-number">5.3.3</span> Stepwise Selection: Backward Stepwise Selection</a></li>
  <li><a href="#stepwise-selection-hybrid-approaches" id="toc-stepwise-selection-hybrid-approaches" class="nav-link" data-scroll-target="#stepwise-selection-hybrid-approaches"><span class="header-section-number">5.3.4</span> Stepwise Selection: Hybrid Approaches</a></li>
  <li><a href="#choosing-the-optimal-model" id="toc-choosing-the-optimal-model" class="nav-link" data-scroll-target="#choosing-the-optimal-model"><span class="header-section-number">5.3.5</span> Choosing the Optimal Model</a></li>
  </ul>
</li>
  <li>
<a href="#shrinkage" id="toc-shrinkage" class="nav-link" data-scroll-target="#shrinkage"><span class="header-section-number">5.4</span> Shrinkage</a>
  <ul>
<li><a href="#ridge-regression" id="toc-ridge-regression" class="nav-link" data-scroll-target="#ridge-regression"><span class="header-section-number">5.4.1</span> Ridge Regression</a></li>
  <li><a href="#lasso-regression" id="toc-lasso-regression" class="nav-link" data-scroll-target="#lasso-regression"><span class="header-section-number">5.4.2</span> Lasso Regression</a></li>
  <li><a href="#coding-example-1" id="toc-coding-example-1" class="nav-link" data-scroll-target="#coding-example-1"><span class="header-section-number">5.4.3</span> Coding example</a></li>
  </ul>
</li>
  <li>
<a href="#dimension-reduction" id="toc-dimension-reduction" class="nav-link" data-scroll-target="#dimension-reduction"><span class="header-section-number">5.5</span> Dimension Reduction</a>
  <ul>
<li><a href="#principal-components-regression-pcr" id="toc-principal-components-regression-pcr" class="nav-link" data-scroll-target="#principal-components-regression-pcr"><span class="header-section-number">5.5.1</span> Principal Components Regression (PCR)</a></li>
  <li><a href="#partial-least-squares-pls" id="toc-partial-least-squares-pls" class="nav-link" data-scroll-target="#partial-least-squares-pls"><span class="header-section-number">5.5.2</span> Partial Least Squares (PLS)</a></li>
  <li><a href="#pcr-vs-pls" id="toc-pcr-vs-pls" class="nav-link" data-scroll-target="#pcr-vs-pls"><span class="header-section-number">5.5.3</span> PCR vs PLS</a></li>
  <li><a href="#coding-example-2" id="toc-coding-example-2" class="nav-link" data-scroll-target="#coding-example-2"><span class="header-section-number">5.5.4</span> Coding example</a></li>
  </ul>
</li>
  <li>
<a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression"><span class="header-section-number">5.6</span> Poisson Regression</a>
  <ul>
<li><a href="#coding-example-3" id="toc-coding-example-3" class="nav-link" data-scroll-target="#coding-example-3"><span class="header-section-number">5.6.1</span> Coding example</a></li>
  </ul>
</li>
  <li>
<a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">5.7</span> Logistic Regression</a>
  <ul>
<li><a href="#estimating-coefficients" id="toc-estimating-coefficients" class="nav-link" data-scroll-target="#estimating-coefficients"><span class="header-section-number">5.7.1</span> Estimating coefficients</a></li>
  <li><a href="#multiple-regression" id="toc-multiple-regression" class="nav-link" data-scroll-target="#multiple-regression"><span class="header-section-number">5.7.2</span> Multiple regression</a></li>
  <li>
<a href="#interpreting-the-model" id="toc-interpreting-the-model" class="nav-link" data-scroll-target="#interpreting-the-model"><span class="header-section-number">5.7.3</span> Interpreting the model</a>
  <ul class="collapse">
<li><a href="#understanding-a-confounding-paradox" id="toc-understanding-a-confounding-paradox" class="nav-link" data-scroll-target="#understanding-a-confounding-paradox">Understanding a confounding paradox</a></li>
  <li><a href="#multinomial-logistic-regression" id="toc-multinomial-logistic-regression" class="nav-link" data-scroll-target="#multinomial-logistic-regression">Multinomial Logistic Regression</a></li>
  </ul>
</li>
  <li><a href="#model-limitatios" id="toc-model-limitatios" class="nav-link" data-scroll-target="#model-limitatios"><span class="header-section-number">5.7.4</span> Model limitatios</a></li>
  <li><a href="#coding-example-4" id="toc-coding-example-4" class="nav-link" data-scroll-target="#coding-example-4"><span class="header-section-number">5.7.5</span> Coding example</a></li>
  </ul>
</li>
  <li>
<a href="#generalized-additive-models-gam" id="toc-generalized-additive-models-gam" class="nav-link" data-scroll-target="#generalized-additive-models-gam"><span class="header-section-number">5.8</span> Generalized Additive Models (GAM)</a>
  <ul>
<li><a href="#gam-regression" id="toc-gam-regression" class="nav-link" data-scroll-target="#gam-regression"><span class="header-section-number">5.8.1</span> GAM Regression</a></li>
  <li><a href="#gam-classification" id="toc-gam-classification" class="nav-link" data-scroll-target="#gam-classification"><span class="header-section-number">5.8.2</span> GAM Classification</a></li>
  <li><a href="#coding-example-5" id="toc-coding-example-5" class="nav-link" data-scroll-target="#coding-example-5"><span class="header-section-number">5.8.3</span> Coding example</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Generalized linear models (GLM)</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><section id="linear-regresion" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="linear-regresion">
<span class="header-section-number">5.1</span> Linear regresion</h2>
<section id="getting-the-least-squares-line-of-a-sample" class="level3" data-number="5.1.1"><h3 data-number="5.1.1" class="anchored" data-anchor-id="getting-the-least-squares-line-of-a-sample">
<span class="header-section-number">5.1.1</span> Getting the Least Squares Line of a sample</h3>
<p>As the <em>population regression line</em> is unobserved the <em>least squares line</em> of a sample is a good estimation. To get it we need to follow the next steps:</p>
<ol type="1">
<li>Define the function to fit.</li>
</ol>
<p><span class="math display">\[
\hat{y} = \hat{\beta}_{0} + \hat{\beta}_{1} x
\]</span></p>
<ol start="2" type="1">
<li>Define how to calculate <strong>residuals</strong>.</li>
</ol>
<p><span class="math display">\[
e_{i} = y_{i} - \hat{y}_{i}
\]</span></p>
<ol start="3" type="1">
<li>Define the <strong>residual sum of squares (RSS)</strong>.</li>
</ol>
<p><span class="math display">\[
RSS = e_{1}^2 + e_{2}^2 + \dots + e_{n}^2
\]</span></p>
<ol start="4" type="1">
<li>Use calculus or make estimation with a computer to find the coefficients that minimize the RSS.</li>
</ol>
<p><span class="math display">\[
\hat{\beta}_{1} = \frac{\Sigma_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}
                       {\Sigma_{i=1}^{n}(x_{i}-\overline{x})}
, \quad
\hat{\beta}_{0} = \overline{y} - \hat{\beta}_{1}\overline{x}
\]</span></p>
</section><section id="getting-confident-intervarls-of-coeffients" class="level3" data-number="5.1.2"><h3 data-number="5.1.2" class="anchored" data-anchor-id="getting-confident-intervarls-of-coeffients">
<span class="header-section-number">5.1.2</span> Getting confident intervarls of coeffients</h3>
<p>To estimate the <strong>population regression line</strong> we can calculate <strong>conﬁdence intervals</strong> for sample coefficients, to define a range where we can find the population values with a defined <strong>confidence level</strong>.</p>
<blockquote class="blockquote">
<p>If we want to use 95% of confidence we need to know that after taking many samples only 95% of the intervals produced with this <strong>confident level</strong> would have the true value (parameter).</p>
</blockquote>
<p>To generate confident intervals we would need to calculate the variance of the <em>random error</em>.</p>
<p><span class="math display">\[
\sigma^2 = Var(\epsilon)
\]</span></p>
<p>But as we can not calculate that variance an alternative can be to estimate it based on residuals if they meet the next conditions:</p>
<ol type="1">
<li>Each residual have common variance <span class="math inline">\(\sigma^2\)</span>, so the variances of the error terms shouldn’t have any relation with the value of the response.</li>
<li>Residuals are uncorrelated. For example, if <span class="math inline">\(\epsilon_{i}\)</span> is positive, that provides little or no information about the sign of <span class="math inline">\(\epsilon_{i+1}\)</span>.</li>
</ol>
<p>If not, we would end underestimating the true standard errors, reducing the probability a given confident level to contain the true value of the parameter and underrating the <em>p-values</em> associated with the model.</p>
<p><span class="math display">\[
\sigma \approx RSE = \sqrt{\frac{RSS}{(n-p-1)}}
\]</span></p>
<p>Now we can calculate the <strong>standard error</strong> of each coefficient and calculate the confident intervals.</p>
<p><span class="math display">\[
SE(\hat{\beta_{0}})^2 = \sigma^2
                       \left[\frac{1}{n}+
                             \frac{\overline{x}^2}
                                  {\Sigma_{i=1}^{n} (x_{i}-\overline{x})^2}
                       \right]
\]</span></p>
<p><span class="math display">\[
SE(\hat{\beta_{1}})^2 = \frac{\sigma^2}
                             {\Sigma_{i=1}^{n} (x_{i} - \overline{x})^2}
\]</span></p>
<p><span class="math display">\[  
\hat{\beta_{1}} \pm 2 \cdot SE(\hat{\beta_{1}}), \quad \hat{\beta_{0}} \pm 2 \cdot SE(\hat{\beta_{0}})
\]</span></p>
</section><section id="insights-to-extract" class="level3" data-number="5.1.3"><h3 data-number="5.1.3" class="anchored" data-anchor-id="insights-to-extract">
<span class="header-section-number">5.1.3</span> Insights to extract</h3>
<section id="confirm-the-relationship-between-the-response-and-predictors" class="level4"><h4 class="anchored" data-anchor-id="confirm-the-relationship-between-the-response-and-predictors">Confirm the relationship between the Response and Predictors</h4>
<p>Use the regression <strong>overall P-value</strong> (based on the F-statistic) to confirm that at <strong>least one predictor</strong> is related with the Response and avoid interpretative problems associated with the number of observations (<em>n</em>) or predictors (<em>p</em>).</p>
<p><span class="math display">\[
H_{0}: \beta_{1} = \beta_{2} = \dots = \beta_{p} = 0
\]</span></p>
<p><span class="math display">\[
H_{a}: \text{at least one } \beta_{j} \text{ is non-zero}
\]</span></p>
</section><section id="accuracy-of-the-model-relationship-strength" class="level4"><h4 class="anchored" data-anchor-id="accuracy-of-the-model-relationship-strength">Accuracy of the model (relationship strength)</h4>
<p>If we want to know how well the model fits to the data we have two options:</p>
<ul>
<li><p><strong>Residual standard error (RSE)</strong>: Even if the model were correct, the actual values of <span class="math inline">\(\hat{y}\)</span> would differ from the true regression line by approximately <em>this units</em>, on average. To get the percentage error we can calculate <span class="math inline">\(RSE/\overline{x}\)</span></p></li>
<li><p><strong>The <span class="math inline">\(R^2\)</span> statistic</strong>: The proportion of variance explained by taking as a reference the <strong>total sum of squares (TSS)</strong>.</p></li>
</ul>
<p><span class="math display">\[
TSS = \Sigma(y_{i} - \overline{y})^2
\]</span></p>
<p><span class="math display">\[
R^2 = \frac{TSS - RSS}{TSS}
\]</span></p>
<p><span class="math display">\[
R^2 =
\begin{cases}
    Cor(X, Y)^2  &amp; \text{Simple Lineal Regresion} \\
    Cor(Y,\hat{Y})^2 &amp; \text{Multipline Lineal Regresion}
\end{cases}
\]</span></p>
</section><section id="confirm-the-relationship-between-the-response-and-each-predictor" class="level4"><h4 class="anchored" data-anchor-id="confirm-the-relationship-between-the-response-and-each-predictor">Confirm the relationship between the Response and each predictor</h4>
<p>To answer that we can test if a particular subset of q of the coefficients are zero.</p>
<p><span class="math display">\[
H_{0}: \beta_{p-q+1} = \beta_{p-q+2} = \dots = \beta_{p} = 0
\]</span></p>
<p>In this case, F-statistic reports the <strong>partial eﬀect</strong> of adding a extra variable to the model (the order matters) to apply a <em>variable selection</em> technique. The classical approach is to:</p>
<ol type="1">
<li>Fit a model for each variable combination <span class="math inline">\(2^p\)</span>.</li>
<li>Select the best model based on <em>Mallow’s Cp</em>, <em>Akaike information criterion (AIC)</em>, <em>Bayesian information criterion (BIC)</em>, and <em>adjusted</em> <span class="math inline">\(R^2\)</span> or plot various model outputs, such as the residuals, in order to search for patterns.</li>
</ol>
<p>But just think that if we have <span class="math inline">\(p = 30\)</span> we will have <span class="math inline">\(2^{30} = =1,073,741,824\ models\)</span> to fit, that it’s too much. Some alternative approaches for this task:</p>
<ul>
<li>Forward selection</li>
<li>Backward selection (cannot be used if p &gt;n)</li>
<li>Mixed selection</li>
</ul></section><section id="size-of-association-between-each-predictor-and-the-response." class="level4"><h4 class="anchored" data-anchor-id="size-of-association-between-each-predictor-and-the-response.">Size of association between each predictor and the response.</h4>
<p>To check that we need to see the <span class="math inline">\(\hat{\beta}_{j}\)</span> <em>confident intervals</em> as the real <span class="math inline">\(\beta_{j}\)</span> is in that range.</p>
</section><section id="predicting-future-values" class="level4"><h4 class="anchored" data-anchor-id="predicting-future-values">Predicting future values</h4>
<p>If we want to predict the average response <span class="math inline">\(f(X)\)</span> we can use the confident intervals, but if we want to predict an individual response <span class="math inline">\(Y = f(X) + \epsilon\)</span> we need to use prediction intervals as they account for the uncertainty associated with <span class="math inline">\(\epsilon\)</span>, the irreducible error.</p>
</section></section><section id="standard-linear-regression-model-assumptions" class="level3" data-number="5.1.4"><h3 data-number="5.1.4" class="anchored" data-anchor-id="standard-linear-regression-model-assumptions">
<span class="header-section-number">5.1.4</span> Standard linear regression model assumptions</h3>
<ul>
<li><p>The <strong>additivity assumption</strong> means that the association between a predictor <span class="math inline">\(X_{j}\)</span> and the response <span class="math inline">\(Y\)</span> does not depend on the values of the other predictors, as it happens when there is a <em>interaction (synergy) effect</em></p></li>
<li><p>The <strong>linearity assumption</strong> states that the change in the response Y associated with a one-unit change in <span class="math inline">\(X_{j}\)</span> is constant, regardless of the value of <span class="math inline">\(X_{j}\)</span>.</p></li>
</ul>
<section id="including-an-interaction-term" class="level4"><h4 class="anchored" data-anchor-id="including-an-interaction-term">Including an interaction term</h4>
<p>This approach relax the <em>additivity assumption</em> that models usually have.</p>
<ul>
<li><strong>2 quantitative variables</strong></li>
</ul>
<p>It consist in adding an extra coefficient which multiplies two or more variables.</p>
<p><span class="math display">\[
\begin{split}
Y &amp; = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \beta_{3} X_{1} X_{2} + \epsilon \\
  &amp; = \beta_{0} + (\beta_{1} + \beta_{3} X_{2}) X_{1} + \beta_{2} X_{2} + \epsilon \\
  &amp; = \beta_{0} + \tilde{\beta}_{1} X_{1} + \beta_{2} X_{2} + \epsilon
\end{split}
\]</span></p>
<p>After adding the interaction term we could interpret the change as making one of the original coefficient a function of the another variable. Now we could say that <span class="math inline">\(\beta_{3}\)</span> <em>represent <strong>the change of</strong></em> <span class="math inline">\(X_{1}\)</span> <em><strong>effectiveness</strong> associated with a one-unit increase in</em> <span class="math inline">\(X_{2}\)</span>.</p>
<p>It very important that we keep <strong>hierarchical principle</strong>, which states that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant as <strong><em>it would alter the meaning of the interaction</em></strong>.</p>
<ul>
<li><strong>1 quantitative and 1 qualitative variable</strong></li>
</ul>
<p>If <span class="math inline">\(X_{1}\)</span> is quantitative and <span class="math inline">\(X_{2}\)</span> is qualitative:</p>
<p><span class="math display">\[
\hat{Y} =
\begin{cases}
    (\beta_{0} + \beta_{2}) + (\beta_{1} + \beta_{3})X_{1} &amp; \text{if }X_{2} \text{ is TRUE}\\
    \beta_{0} + \beta_{1}X_{1}                             &amp; \text{if }X_{2} \text{ is FALSE}
\end{cases}
\]</span></p>
<p>Adding the <span class="math inline">\(\beta_{3}\)</span> interaction allow the line to change the line slope based on <span class="math inline">\(X_{2}\)</span> and not just a different intercept.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/03-factor-interaction.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="polynomial-regression" class="level4"><h4 class="anchored" data-anchor-id="polynomial-regression">Polynomial regression</h4>
<p>This approach relax the <em>linearity assumption</em> that models usually have. It consist in including transformed versions of the predictors.</p>
<p><span class="math display">\[
\begin{split}
Y &amp; = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} \\
  &amp; = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{1}^2
\end{split}
\]</span></p>
</section></section><section id="possible-problems" class="level3" data-number="5.1.5"><h3 data-number="5.1.5" class="anchored" data-anchor-id="possible-problems">
<span class="header-section-number">5.1.5</span> Possible problems</h3>
<section id="the-target-doesnt-follow-a-normal-distribution" class="level4"><h4 class="anchored" data-anchor-id="the-target-doesnt-follow-a-normal-distribution">The target doesn’t follow a normal distribution</h4>
<p>We have two ways to detect this problem:</p>
<ol type="1">
<li>Plot the target variable with a histogram</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/46-ames-sale-price-distribution.png" class="img-fluid figure-img"></p>
</figure>
</div>
<ol start="2" type="1">
<li>Plotting the residual and checking the plot creates a funnel shape, demonstrating a <em>non-constant variance (heteroscedasticity) of error terms</em>.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/05-non-constance-variance.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>To solve this problem we can apply transformations to the target variable.</p>
<ol type="1">
<li>Applying a concave function such as <span class="math inline">\(\log{Y}\)</span> or <span class="math inline">\(\sqrt{Y}\)</span>. If the target value has 0s we can define the argument offset as 1.</li>
</ol>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ames_recipe</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">recipe</span><span class="op">(</span><span class="va">Sale_Price</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">ames_train</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">step_log</span><span class="op">(</span><span class="fu">all_outcomes</span><span class="op">(</span><span class="op">)</span>, offset <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Applying <strong>Box Cox transformation</strong> is more flexible than the log transformation and will find an appropriate transformation from a family of power transforms that will <strong>transform the variable as close as possible to a normal distribution</strong>. Its lambda (<span class="math inline">\(\lambda\)</span>) value varies from -5 to 5 and it’s selected based on the traning data.</li>
</ol>
<p><span class="math display">\[
y(\lambda) =
\begin{cases}
\frac{Y^\lambda - 1}{\lambda}, \text{if } \lambda \neq 0 \\
\log{(Y)}, \text{if } \lambda = 0
\end{cases}
\]</span></p>
<ol start="3" type="1">
<li>If your data consists of values <span class="math inline">\(y \leq 1\)</span> use the <strong>Yeo-Johnson transformation</strong>, which is very similar to the Box-Cox one.</li>
</ol>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ames_recipe</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">recipe</span><span class="op">(</span><span class="va">Sale_Price</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">ames_train</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">step_YeoJohnson</span><span class="op">(</span><span class="fu">all_outcomes</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To make correct interpretations of our predictions it’s important to remember that we need to <strong>revert this transformation after getting the prediction</strong>.</p>
<p>This actions will reduce the <strong>relative error</strong> of each prediction.</p>
</section><section id="collinearity" class="level4"><h4 class="anchored" data-anchor-id="collinearity">Collinearity</h4>
<p><strong>Collinearity</strong> refers to the situation in which two or more predictor variables are closely related (highly correlated) to one another. It reduces the accuracy of the estimates of the regression coeﬃcients and causes the standard error for <span class="math inline">\(\hat{\beta}_{j}\)</span> to grow. That reduce the <strong>power of the hypothesis test</strong>,that is, the probability of correctly detecting a non-zero coeﬃcient.</p>
<p>Looking at the correlation matrix of the predictors could be usefull, but it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation (<strong>multicollinearity</strong>).</p>
<table class="table">
<colgroup>
<col style="width: 64%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Detection method</th>
<th style="text-align: left;">Solutions</th>
</tr></thead>
<tbody><tr class="odd">
<td style="text-align: left;">The best way to assess multicollinearity is to compute the <strong>variance inﬂation factor (VIF)</strong>, which is the ratio of the variance of <span class="math inline">\(\hat{\beta}_{j}\)</span> when ﬁtting the full model divided by the variance of <span class="math inline">\(\hat{\beta}_{j}\)</span> if ﬁt on its own with 1 as its lowest value and 5 or 10 as problematic values of collinearity</td>
<td style="text-align: left;">1. Drop one of the problematic variables from the regression.   2. Combine the collinear variables together into a single predictor</td>
</tr></tbody>
</table>
<p><span class="math display">\[
\text{VIF}(\hat{\beta}_{j}) = \frac{1}
                                   {1 - R_{X_{j}|X_{-j}}^2}
\]</span></p>
<p>Where <span class="math inline">\(R_{X_{j}|X_{-j}}^2\)</span> is the <span class="math inline">\(R^2\)</span> from a regression of <span class="math inline">\(X_{j}\)</span> onto all of the other predictors.</p>
</section><section id="non-linearity-of-the-response-predictor-relationships" class="level4"><h4 class="anchored" data-anchor-id="non-linearity-of-the-response-predictor-relationships">Non-linearity of the response-predictor relationships</h4>
<table class="table">
<colgroup>
<col style="width: 64%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Detection method</th>
<th style="text-align: left;">Solutions</th>
</tr></thead>
<tbody><tr class="odd">
<td style="text-align: left;">Plot the <strong>residuals versus predicted values</strong> <span class="math inline">\(\hat{y}_{i}\)</span>. Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.</td>
<td style="text-align: left;">A simple approach is to use non-linear transformations of the predictors, such as <span class="math inline">\(\log{X}\)</span>, <span class="math inline">\(\sqrt{X}\)</span>, and <span class="math inline">\(X^2\)</span>, in the regression model</td>
</tr></tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/04-residuals-predicted-values.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="correlation-of-error-terms" class="level4"><h4 class="anchored" data-anchor-id="correlation-of-error-terms">Correlation of error terms</h4>
<p>If there is correlation among the errors, then the estimated standard errors of the coefficients will be biased <strong>leading to prediction intervals being narrower</strong> than they should be.</p>
<table class="table">
<colgroup>
<col style="width: 64%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Detection method</th>
<th style="text-align: left;">Solutions</th>
</tr></thead>
<tbody><tr class="odd">
<td style="text-align: left;">1. Plot the residuals from our model as a function of time or execution order. If the errors are uncorrelated, then there should be no discernible pattern.   2. Check if some observation have been exposed to the same environmental factors</td>
<td style="text-align: left;">Good experimental design is crucial in order to mitigate these problems</td>
</tr></tbody>
</table></section><section id="outliers" class="level4"><h4 class="anchored" data-anchor-id="outliers">Outliers</h4>
<p>An outlier is a point for which <span class="math inline">\(y_{i}\)</span> is far from the value predicted by the model. Sometimes, they have little effect on the least squares line, but <em>over estimate the RSE</em> making bigger p-values of the model and <em>under estimate the</em> <span class="math inline">\(R^2\)</span>.</p>
<table class="table">
<colgroup>
<col style="width: 64%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Detection method</th>
<th style="text-align: left;">Solutions</th>
</tr></thead>
<tbody><tr class="odd">
<td style="text-align: left;">Plot the <strong>studentized residuals</strong>, computed by dividing each residual <span class="math inline">\(e_{i}\)</span> by its estimated standard error. Then search for points which absolute value is greater than 3</td>
<td style="text-align: left;">They can be removed if it has occurred due to an error in data collection. Otherwise, they may indicate a deficiency with the model, such as a missing predictor.</td>
</tr></tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/06-studentized-residuals-plot.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="high-leverage-points" class="level4"><h4 class="anchored" data-anchor-id="high-leverage-points">High-leverage points</h4>
<p>Observations with <strong>high leverage</strong> have an unusual value for <span class="math inline">\(x_{i}\)</span>. High leverage observations tend to have a sizable impact on the estimated regression line and any problems with these points may invalidate the entire fit.</p>
<table class="table">
<colgroup>
<col style="width: 64%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Detection method</th>
<th style="text-align: left;">Solutions</th>
</tr></thead>
<tbody><tr class="odd">
<td style="text-align: left;">Compute the leverage statistic. Find an observation with higher value than mean, represented by <span class="math inline">\((p + 1)/n\)</span>. Leverage values are always between <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>
</td>
<td style="text-align: left;">Make sure that the value is correct and not a data collection problem</td>
</tr></tbody>
</table>
<p><span class="math display">\[
h_{i} = \frac{1}{n} +
        \frac{(x_{i} - \overline{x})^2}
              {\Sigma_{i'=1}^n(x_{i'} - \overline{x})^2}
\]</span></p>
<p>In a <em>multiple linear regression</em>, it is possible to have an observation that is well within the range of each individual predictor’s values, but that is unusual in terms of the full set of predictors.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/07-studentized-residuals-leverage-plot.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section></section><section id="avoid-using-for-classification-problems" class="level3" data-number="5.1.6"><h3 data-number="5.1.6" class="anchored" data-anchor-id="avoid-using-for-classification-problems">
<span class="header-section-number">5.1.6</span> Avoid using for classification problems</h3>
<p>There are better model to achieve that kind of situation. For example, he linear <strong>discriminant analysis (LDA)</strong> procedure the same response of a linear regression for a binary problem. Other reasons are:</p>
<ul>
<li>A regression method cannot accommodate a qualitative response with more than two classes.</li>
<li>A regression method will not provide meaningful estimates of <span class="math inline">\(Pr(Y|X)\)</span> as some of our estimates might be outside the [0, 1] interval.</li>
</ul></section><section id="benefits-of-replacing-plain-least-squares-ﬁtting" class="level3" data-number="5.1.7"><h3 data-number="5.1.7" class="anchored" data-anchor-id="benefits-of-replacing-plain-least-squares-ﬁtting">
<span class="header-section-number">5.1.7</span> Benefits of Replacing plain least squares ﬁtting</h3>
<ul>
<li><p><strong>Prediction Accuracy:</strong> If n is not much larger than p, then there can be a lot of variability in the least squares ﬁt, resulting in overﬁtting and consequently poor predictions on future observations not used in model training. And if p &gt;n, then there is no longer a unique least squares coeﬃcient estimate: the variance is inﬁnite so the method cannot be used at all. As an alternative, We could reduce the variance by increasing in bias (<em>constraining</em> and <em>shrinking</em>).</p></li>
<li><p><strong>Model Interpretability:</strong>There are some methods that can exclude irrelevant variables from a multiple regression model (<em>feature selection</em> or <em>variable selection</em>).</p></li>
</ul></section><section id="coding-example" class="level3" data-number="5.1.8"><h3 data-number="5.1.8" class="anchored" data-anchor-id="coding-example">
<span class="header-section-number">5.1.8</span> Coding example</h3>
<p>To perform <strong>Linear Regression</strong> we just need to create the model specification by using <strong>lm</strong> engine.</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">BikeshareSpit</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">Bikeshare</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lm_spec</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"lm"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lm_rec_spec</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">recipe</span><span class="op">(</span><span class="va">bikers</span> <span class="op">~</span> <span class="va">mnth</span> <span class="op">+</span> <span class="va">hr</span> <span class="op">+</span> <span class="va">workingday</span> <span class="op">+</span> <span class="va">temp</span> <span class="op">+</span> <span class="va">weathersit</span>,</span>
<span>         data <span class="op">=</span> <span class="va">Bikeshare</span> <span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_dummy</span><span class="op">(</span><span class="fu">all_nominal_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">add_model</span><span class="op">(</span><span class="va">lm_spec</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">lm_rec_spec</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">last_fit</span><span class="op">(</span>split <span class="op">=</span> <span class="va">BikeshareSpit</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">collect_metrics</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  .metric .estimator .estimate .config             
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 rmse    standard      76.1   Preprocessor1_Model1
2 rsq     standard       0.671 Preprocessor1_Model1</code></pre>
</div>
</div>
</section></section><section id="extending-linear-regression-data-transformations" class="level2" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="extending-linear-regression-data-transformations">
<span class="header-section-number">5.2</span> Extending Linear Regression (Data Transformations)</h2>
<section id="polynomial-regression-1" class="level3" data-number="5.2.1"><h3 data-number="5.2.1" class="anchored" data-anchor-id="polynomial-regression-1">
<span class="header-section-number">5.2.1</span> Polynomial regression</h3>
<p>It extends the <em>linear model</em> by adding extra predictors, obtained by <strong>raising each of the original predictors to a power</strong>.</p>
<p>As result, if the response is a numeric variable we can fit our model to follow the next form:</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/35-polynomial-4degree-regression.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>On the other hand, we can use the <em>logistic regression</em> and apply the same structure to predict the probability of particular class:</p>
<p><span class="math display">\[
\Pr(y_i &gt; 250|x_i) = \frac{\exp(\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d)}
                          {1 + \exp(\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \dots + \beta_d x_i^d)}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/36-polynomial-4degree-logistic-regression.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="piecewise-constant-regression" class="level3" data-number="5.2.2"><h3 data-number="5.2.2" class="anchored" data-anchor-id="piecewise-constant-regression">
<span class="header-section-number">5.2.2</span> Piecewise constant regression</h3>
<p>They <strong>cut the range of a variable into K distinct regions</strong> (known as <em>bins</em>) in order to produce a qualitative variable. This has the eﬀect of ﬁtting a piecewise constant function.</p>
<p>If we define the cutpoints as <span class="math inline">\(c_1, c_2, \dots, c_K\)</span> in the range of <em>X</em>, we can create <em>dummy variables</em> to represent each range. For example, if <span class="math inline">\(c_1 \leq x_i &lt; c_2\)</span> is <code>TRUE</code> then <span class="math inline">\(C_1(x_i) = 1\)</span> and then we need to repeat that process for each value of <span class="math inline">\(X\)</span> and range. As result we can fit a <em>lineal regression</em> based on the new variables.</p>
<p><span class="math display">\[
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i)  \dots + \beta_K C_K(x_i) + \epsilon_i
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/37-step-function-regression.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>On the other hand, we can use the <em>logistic regression</em> and apply the same structure to predict the probability of particular class:</p>
<p><span class="math display">\[
\Pr(y_i &gt; 250|x_i) = \frac{\exp(\beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i)  \dots + \beta_K C_K(x_i)}
                          {1 + \exp(\beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i)  \dots + \beta_K C_K(x_i))}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/38-step-function-logistic-regression.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="piecewise-polynomials-regression-natural-spine" class="level3" data-number="5.2.3"><h3 data-number="5.2.3" class="anchored" data-anchor-id="piecewise-polynomials-regression-natural-spine">
<span class="header-section-number">5.2.3</span> Piecewise polynomials regression (Natural Spine)</h3>
<p>It consist in ﬁtting separate low-degree polynomials over diﬀerent regions of <em>X</em>. For example, a <strong>piecewise cubic polynomial</strong> with a single knot at a point <em>c</em> takes the form.</p>
<p><span class="math display">\[
y_i =
  \begin{cases}
    \beta_{01} + \beta_{11} x_i + \beta_{21} x_i^2 + \beta_{31} x_i^3 + \epsilon_i &amp;  \text{if } x_i&lt;c\\
    \beta_{02} + \beta_{12} x_i + \beta_{22} x_i^2 + \beta_{32} x_i^3 + \epsilon_i &amp;  \text{if } x_i \geq c
  \end{cases}
\]</span></p>
<p>As each polynomial has four parameters, we are using a total of <strong>8 degrees of freedom</strong> in ﬁtting that model. By using that model with <code>Wage</code>data, we can see a problem as the model used was <strong>too flexible</strong> and to solve it we need to <strong>constrain</strong> it to be continuous at <code>age = 50</code>.</p>
<p><img src="img/39-piecewise-cubic-example.png" class="img-fluid" data-fig-align="center"> But as you could see after applying the <strong>continuity constraint</strong> the plot still present an unnatural V-shape that can solve by apply the <strong>continuity constraint</strong> to the <em>first</em> and <em>second</em> derivative of the function, the end with <strong>5 degrees of freedom</strong> model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/40-constrained-piecewise-cubic-example.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In this context, a <strong>natural spline</strong> refers to a <em>regression spline</em> with the additional constraints of maintaining <strong>linearity at the boundaries</strong>.</p>
</section></section><section id="subset-selection" class="level2" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="subset-selection">
<span class="header-section-number">5.3</span> Subset Selection</h2>
<p>This approach involves identifying a subset of the <em>p</em> predictors that we believe to be related to the response. We then ﬁt a model using least squares on the reduced set of variables.</p>
<section id="best-subset-selection" class="level3" data-number="5.3.1"><h3 data-number="5.3.1" class="anchored" data-anchor-id="best-subset-selection">
<span class="header-section-number">5.3.1</span> Best Subset Selection</h3>
<p>It ﬁts all <em>p</em> models that contain exactly one predictor, all <span class="math inline">\(\left( \begin{array}{c} p \\ 2 \end{array} \right) = p (p-1)/2\)</span> models that contain exactly two predictors, and so forth. Then it selects the <em>best</em> model based on smallest RSS or the largest <span class="math inline">\(R^2\)</span>.</p>
<ul>
<li>
<strong>Algorithm 6.1</strong>
<ol type="1">
<li>Let <span class="math inline">\(\mathcal{M}_0\)</span> denote the null model, which represent the sample mean for each observation.</li>
<li>For <span class="math inline">\(k = 1, 2, \dots, p\)</span>:</li>
</ol>
<ul>
<li>Fit all <span class="math inline">\(\left( \begin{array}{c} p \\ k \end{array} \right)\)</span> models that contain exactly k predictors.</li>
<li>Pick the best among these <span class="math inline">\(\left( \begin{array}{c} p \\ k \end{array} \right)\)</span> models using the smallest RSS or the <em>deviance</em> for classification (negative two times the maximized log-likelihood), and call it <span class="math inline">\(\mathcal{M}_k\)</span>
</li>
</ul>
<ol start="3" type="1">
<li>Select a single best model from among <span class="math inline">\(\mathcal{M}_0, \dots, \mathcal{M}_p\)</span> using cross- validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</li>
</ul>
<p>This method is really computational expensive as it needs to fit <span class="math inline">\(2^p\)</span> models. Just think that if your data has 20 predicts, then there are over one million possibilities. Thus an enormous search space can also lead to <strong>overﬁtting</strong> and <strong>high variance of the coeﬃcient estimates</strong>.</p>
</section><section id="stepwise-selection-forward-stepwise-selection" class="level3" data-number="5.3.2"><h3 data-number="5.3.2" class="anchored" data-anchor-id="stepwise-selection-forward-stepwise-selection">
<span class="header-section-number">5.3.2</span> Stepwise Selection: Forward Stepwise Selection</h3>
<p>It begins with a model containing no predictors, and then adds the predictors who gives the greatest additional improvement to the ﬁt, one-at-a-time, until all of the predictors are in the model, as result we will need to fit <span class="math inline">\(1+p(p+1)/2\)</span> models.</p>
<ul>
<li>
<strong>Algorithm 6.2</strong>
<ol type="1">
<li>Let <span class="math inline">\(\mathcal{M}_0\)</span> denote the null model, which represent the sample mean for each observation.</li>
<li>For <span class="math inline">\(k = 0, \dots, p-1\)</span>:</li>
</ol>
<ul>
<li>Consider all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(\mathcal{M}_k\)</span> with one additional predictor.</li>
<li>Choose the best among these <span class="math inline">\(p-k\)</span> models, and call it <span class="math inline">\(\mathcal{M}_{k+1}\)</span>. Here best is deﬁned as having smallest RSS.</li>
</ul>
<ol start="3" type="1">
<li>Select a single best model from among <span class="math inline">\(\mathcal{M}_k, \dots, \mathcal{M}_p\)</span> using cross- validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</li>
</ul>
<p>Though it tends to do well in practice, it is not guaranteed to ﬁnd the best possible model out of all <span class="math inline">\(2^p\)</span> models containing subsets of the <em>p</em> predictors, but can be used even if <span class="math inline">\(n &lt; p\)</span>.</p>
</section><section id="stepwise-selection-backward-stepwise-selection" class="level3" data-number="5.3.3"><h3 data-number="5.3.3" class="anchored" data-anchor-id="stepwise-selection-backward-stepwise-selection">
<span class="header-section-number">5.3.3</span> Stepwise Selection: Backward Stepwise Selection</h3>
<p>It begins with the full least squares model containing all <em>p</em> predictors, and then iteratively removes the least useful predictor, one-at-a-time. As result we will need to fit <span class="math inline">\(1+p(p+1)/2\)</span> models</p>
<ul>
<li>
<strong>Algorithm 6.3</strong>
<ol type="1">
<li>Let <span class="math inline">\(\mathcal{M}_p\)</span> denote the full model, which contains all <em>p</em> predictors.</li>
<li>For <span class="math inline">\(k = p, p -1, \dots , 1\)</span>:</li>
</ol>
<ul>
<li>Consider all <em>k</em> models that contain all but one of the predictors in <span class="math inline">\(\mathcal{M}_k\)</span>, for a total of <span class="math inline">\(k-1\)</span> predictors.</li>
<li>Choose the best among these <em>k</em> models, and call it <span class="math inline">\(\mathcal{M}_{k-1}\)</span>. Here best is deﬁned as having smallest RSS.</li>
</ul>
<ol start="3" type="1">
<li>Select a single best model from among <span class="math inline">\(\mathcal{M}_0, \dots, \mathcal{M}_p\)</span> using cross- validated prediction error, <span class="math inline">\(C_p\)</span> (AIC), BIC or adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</li>
</ul>
<p>This method don’t guarantee to yield the best model containing a subset of the <em>p</em> predictors and we need to cofirm that <span class="math inline">\(n \geq p\)</span>.</p>
</section><section id="stepwise-selection-hybrid-approaches" class="level3" data-number="5.3.4"><h3 data-number="5.3.4" class="anchored" data-anchor-id="stepwise-selection-hybrid-approaches">
<span class="header-section-number">5.3.4</span> Stepwise Selection: Hybrid Approaches</h3>
<p>In this method variables adds variables sequentially, but after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model ﬁt. Such an approach attempts to more closely mimic <em>Best Subset Selection</em> while retaining the computational advantages of <em>forward</em> and <em>backward stepwise selection</em>.</p>
</section><section id="choosing-the-optimal-model" class="level3" data-number="5.3.5"><h3 data-number="5.3.5" class="anchored" data-anchor-id="choosing-the-optimal-model">
<span class="header-section-number">5.3.5</span> Choosing the Optimal Model</h3>
<p>As the model which contains all of the predictors will always have the smallest RSS, we need to estimate the test error rate by:</p>
<ul>
<li>Making an adjustment to the training error to account for the bias due to overﬁtting.</li>
<li>Using either a validation set approach or a cross-validation approach.</li>
</ul>
<p>Let’s see the methods that relay on correcting the training error:</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 29%">
<col style="width: 45%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Formula</th>
<th style="text-align: left;">Interpretation</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(C_p\)</span></td>
<td style="text-align: center;"><span class="math inline">\(C_p = \frac{1}{n} (\text{RSS} + 2d\hat{\sigma}^2)\)</span></td>
<td style="text-align: left;">
<span class="math inline">\(2d\hat{\sigma}^2\)</span> represent the penalty of adding new predictors. This method is a good estimation of <strong>test MSE</strong> if the <span class="math inline">\(\hat{\sigma}^2\)</span> is an unbiased estimate of the <span class="math inline">\(\sigma^2\)</span>
</td>
</tr>
<tr class="even">
<td style="text-align: center;">Akaike information criterion</td>
<td style="text-align: center;"><span class="math inline">\(\text{AIC} = \frac{1}{n} (\text{RSS} + 2d\hat{\sigma}^2)\)</span></td>
<td style="text-align: left;">The book omits irrelevant constants to show that <span class="math inline">\(C_p\)</span> and <span class="math inline">\(\text{AIC}\)</span> are proportional to each other</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Bayesian information criterion</td>
<td style="text-align: center;"><span class="math inline">\(\text{BIC} = \frac{1}{n}(\text{RSS} +\log(n) d \hat{\sigma}^2)\)</span></td>
<td style="text-align: left;">After omitting irrelevant constants, we can see that <span class="math inline">\(\log n &gt; 2\)</span> for any <span class="math inline">\(n &gt; 7\)</span>, as consequence, the metric trends to add a heavier penalty on models with many variables than the <span class="math inline">\(C_p\)</span> and tent to select models with fewer predictors</td>
</tr>
<tr class="even">
<td style="text-align: center;">Adjusted <span class="math inline">\(R^2\)</span>
</td>
<td style="text-align: center;"><span class="math inline">\(\text{Adjusted} \; R^2 = 1 - \frac{\text{RSS}/(n - d -1)}{\text{TSS}/(n-1)}\)</span></td>
<td style="text-align: left;">A large value of adjusted <span class="math inline">\(R^2\)</span> indicates a model with a small test error, even though the metric doesn’t rely on rigorous theoretical justiﬁcations.</td>
</tr>
</tbody>
</table>
<p><em>Where:</em></p>
<ul>
<li>
<span class="math inline">\(n\)</span>: Number of observations</li>
<li>
<span class="math inline">\(d\)</span>: Number of predictors</li>
<li>
<span class="math inline">\(\hat{\sigma}^2\)</span>: Estimate of the variance of the error <span class="math inline">\(\epsilon\)</span> associated with each response</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/25-cp-bic-adj-r2-example.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
</section></section><section id="shrinkage" class="level2" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="shrinkage">
<span class="header-section-number">5.4</span> Shrinkage</h2>
<p>This approach involves ﬁtting a model involving all <em>p</em> predictors and shrinks the estimated coeﬃcients towards zero. Depending on what type of shrinkage is performed, some of the coeﬃcients may be estimated exactly at zero, performing some variable selection.</p>
<section id="ridge-regression" class="level3" data-number="5.4.1"><h3 data-number="5.4.1" class="anchored" data-anchor-id="ridge-regression">
<span class="header-section-number">5.4.1</span> Ridge Regression</h3>
<p>The method rather than using RSS as the metric to minimize with the regression coefficient <span class="math inline">\(\beta_0, \beta_1, \dots, \beta_p\)</span>, it is modified by adding a <strong>shrinkage penalty</strong> that the effect of estimating <span class="math inline">\(\beta_j\)</span> towards zero when the coefficient is close to 0:</p>
<p><span class="math display">\[
RSS + \lambda \sum_{j=1}^p \beta_j^2
\]</span></p>
<p><em>Where:</em></p>
<ul>
<li>
<span class="math inline">\(\lambda\)</span>: It’s a tuning parameter <span class="math inline">\(\geq 0\)</span> that can be calculated using cross-validation.</li>
<li>
<span class="math inline">\(\text{RSS}\)</span>: Present the residual standard error.</li>
</ul>
<p>As result, the ridge regression will produce a diﬀerent set of coeﬃcient estimates for each <span class="math inline">\(\lambda\)</span> value, <span class="math inline">\(\hat{\beta}_\lambda^R\)</span>. By plotting the new coefficients against the <em>penalty</em> used we can see how the coefficients move towards zero without reaching the absolute 0, but may also be useful to plot the <span class="math inline">\(\ell_2 \; \mathcal{norm}\)</span> (<span class="math inline">\(\| \beta \|_2 = \sqrt{\sum_{j=1}^p \beta^2}\)</span>) proportion of the <em>ridge</em> vs <em>least squares</em> coefficients, to compute the <em>proportion</em> in which the ridge regression coeﬃcient estimate have been shrunken towards zero.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/26-ridge-regression-change.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
<p>As the <span class="math inline">\(\lambda\)</span> to select is sensible to the predictors scaling, it’s a good practice scaling the predictors using the next formula, to make all the predictors to have an standard deviation of 1:</p>
<p><span class="math display">\[
\tilde{x}_{ij} =
\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}} =
\frac{x_{ij}}{\sigma_j}
\]</span></p>
<p>When the number of variables <em>p</em> is almost as large as the number of observations <em>n</em> the least squares estimates will be extremely variable and this method <strong>increase the bias</strong> by reducing the <em>flexibility</em> through <span class="math inline">\(\lambda\)</span> to <strong>reduce the variance</strong> and find the lower error rate, without making many computations as happens the <em>best subset selection</em> method.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/27-ridge-regression-bias-variance-trade-off.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
</section><section id="lasso-regression" class="level3" data-number="5.4.2"><h3 data-number="5.4.2" class="anchored" data-anchor-id="lasso-regression">
<span class="header-section-number">5.4.2</span> Lasso Regression</h3>
<p><em>Ridge Regression</em> don’t set the coefficient exactly to zero (unless <span class="math inline">\(\lambda = \infty\)</span>). That doesn’t affect the model accuracy but doesn’t provide any help when we have to interpret a model with many predicts. To over come that problem, the <em>Lasso Regression</em> performs <strong><em>variable selection</em></strong> based on minimization of the next function:</p>
<p><span class="math display">\[
RSS + \lambda \sum_{j=1}^p |\beta_j|
\]</span></p>
<p><em>Where:</em></p>
<ul>
<li>
<span class="math inline">\(\lambda\)</span>: It’s a tuning parameter <span class="math inline">\(\geq 0\)</span> that can be calculated using cross-validation.</li>
<li>
<span class="math inline">\(\text{RSS}\)</span>: Present the residual standard error.</li>
</ul>
<p>As consequence, the method uses the <span class="math inline">\(\ell_1\)</span> penalty (<span class="math inline">\(\| \beta \|_1 = \sum_{j=1}^p |\beta|\)</span>), instead of the <span class="math inline">\(\ell_2\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/28-lasso-regression-change.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
</section><section id="coding-example-1" class="level3" data-number="5.4.3"><h3 data-number="5.4.3" class="anchored" data-anchor-id="coding-example-1">
<span class="header-section-number">5.4.3</span> Coding example</h3>
<p>To perform a <strong>Ridge</strong> or <strong>Lasso</strong> regression we need to use the function <code>linear_reg</code> and define the <code>mixture</code> argument depending on the regression we want to perform.</p>
<table class="table">
<thead><tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Mixture</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Ridge Regression</td>
<td style="text-align: left;">mixture = <strong>0</strong>
</td>
</tr>
<tr class="even">
<td style="text-align: left;">Lasso Regression</td>
<td style="text-align: left;">mixture = <strong>1</strong>
</td>
</tr>
</tbody>
</table>
<p>As both regression depend on the <code>penalty</code> parameter we need to use cross-validation to estimate the best one. But, if you want to explore the parameter by yourself you has the next options.</p>
<ul>
<li>Fit a model and explore different penalties using <code>tidy</code>, <code>augment</code>, <code>predict</code> or <code>autoplot</code> functions.</li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Hitters</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">as_tibble</span><span class="op">(</span><span class="va">Hitters</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">Salary</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">ridge_fit</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">linear_reg</span><span class="op">(</span>mixture <span class="op">=</span> <span class="fl">0</span>, penalty <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">fit</span><span class="op">(</span><span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Hitters</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">tidy</span><span class="op">(</span><span class="va">ridge_fit</span>, penalty <span class="op">=</span> <span class="fl">11498</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 20 × 3
   term         estimate penalty
   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;
 1 (Intercept) 407.        11498
 2 AtBat         0.0370    11498
 3 Hits          0.138     11498
 4 HmRun         0.525     11498
 5 Runs          0.231     11498
 6 RBI           0.240     11498
 7 Walks         0.290     11498
 8 Years         1.11      11498
 9 CAtBat        0.00314   11498
10 CHits         0.0117    11498
11 CHmRun        0.0876    11498
12 CRuns         0.0234    11498
13 CRBI          0.0242    11498
14 CWalks        0.0250    11498
15 LeagueN       0.0866    11498
16 DivisionW    -6.23      11498
17 PutOuts       0.0165    11498
18 Assists       0.00262   11498
19 Errors       -0.0206    11498
20 NewLeagueN    0.303     11498</code></pre>
</div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridge_fit</span>, new_data <span class="op">=</span> <span class="va">Hitters</span>, penalty <span class="op">=</span> <span class="fl">11498</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 263 × 1
   .pred
   &lt;dbl&gt;
 1  533.
 2  553.
 3  620.
 4  487.
 5  559.
 6  440.
 7  446.
 8  453.
 9  620.
10  615.
# ℹ 253 more rows</code></pre>
</div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">ridge_fit</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">scale_x_log10</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org/reference/comma.html">comma_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">theme_light</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="05-generalized-linear-models_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s have a example using the tidymodels approach. By following the next steps:</p>
<ol type="1">
<li>Splitting the data in testing and training data.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">18</span><span class="op">)</span></span>
<span><span class="va">Hitters_split</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">Hitters</span>, strata <span class="op">=</span> <span class="st">"Salary"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Hitters_train</span> <span class="op">&lt;-</span> <span class="fu">training</span><span class="op">(</span><span class="va">Hitters_split</span><span class="op">)</span></span>
<span><span class="va">Hitters_test</span> <span class="op">&lt;-</span> <span class="fu">testing</span><span class="op">(</span><span class="va">Hitters_split</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Define the workflow to use.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ridge_recipe</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">recipe</span><span class="op">(</span>formula <span class="op">=</span> <span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Hitters_train</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_novel</span><span class="op">(</span><span class="fu">all_nominal_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_dummy</span><span class="op">(</span><span class="fu">all_nominal_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_zv</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">ridge_spec</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">linear_reg</span><span class="op">(</span>penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>, mixture <span class="op">=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">ridge_workflow</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">ridge_recipe</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_model</span><span class="op">(</span><span class="va">ridge_spec</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="3" type="1">
<li>Use cross-validation to estimate the testing error and tune the penalty.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">40</span><span class="op">)</span></span>
<span><span class="va">Hitters_fold</span> <span class="op">&lt;-</span> <span class="fu">vfold_cv</span><span class="op">(</span><span class="va">Hitters_train</span>, v <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="4" type="1">
<li>Define the penalties to check. Where the range indicates the limit of <span class="math inline">\(x\)</span> in the function <span class="math inline">\(10^x\)</span> and the level the number of step to complete the range.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">penalty_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">penalty</span><span class="op">(</span>range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="5" type="1">
<li>Let’s fit our models.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tune_res</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">tune_grid</span><span class="op">(</span><span class="va">ridge_workflow</span>,</span>
<span>            resamples <span class="op">=</span> <span class="va">Hitters_fold</span>, </span>
<span>            grid <span class="op">=</span> <span class="va">penalty_grid</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="6" type="1">
<li>Check the test error change in a plot or just export a table.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">tune_res</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">scale_x_log10</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org/reference/comma.html">comma_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">theme_light</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="05-generalized-linear-models_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">collect_metrics</span><span class="op">(</span><span class="va">tune_res</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">.metric</span> <span class="op">==</span> <span class="st">"rsq"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">arrange</span><span class="op">(</span><span class="fu">desc</span><span class="op">(</span><span class="va">mean</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 50 × 7
   penalty .metric .estimator  mean     n std_err .config              
     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
 1    910. rsq     standard   0.454    10  0.0647 Preprocessor1_Model40
 2    569. rsq     standard   0.454    10  0.0646 Preprocessor1_Model39
 3   1456. rsq     standard   0.453    10  0.0646 Preprocessor1_Model41
 4    356. rsq     standard   0.453    10  0.0644 Preprocessor1_Model38
 5   2330. rsq     standard   0.452    10  0.0645 Preprocessor1_Model42
 6    222. rsq     standard   0.450    10  0.0640 Preprocessor1_Model37
 7   3728. rsq     standard   0.450    10  0.0643 Preprocessor1_Model43
 8   5964. rsq     standard   0.449    10  0.0642 Preprocessor1_Model44
 9   9541. rsq     standard   0.448    10  0.0641 Preprocessor1_Model45
10    139. rsq     standard   0.448    10  0.0634 Preprocessor1_Model36
# ℹ 40 more rows</code></pre>
</div>
</div>
<ol start="7" type="1">
<li>Select the best model configuration and update the workflow to use that parameter.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">best_penalty</span> <span class="op">&lt;-</span> <span class="fu">select_best</span><span class="op">(</span><span class="va">tune_res</span>, metric <span class="op">=</span> <span class="st">"rsq"</span><span class="op">)</span></span>
<span><span class="va">best_penalty</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
  penalty .config              
    &lt;dbl&gt; &lt;chr&gt;                
1    910. Preprocessor1_Model40</code></pre>
</div>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ridge_final</span> <span class="op">&lt;-</span> <span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">ridge_workflow</span>, <span class="va">best_penalty</span><span class="op">)</span></span>
<span><span class="va">ridge_final</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>══ Workflow ════════════════════════════════════════════════════════════════════
Preprocessor: Recipe
Model: linear_reg()

── Preprocessor ────────────────────────────────────────────────────────────────
4 Recipe Steps

• step_novel()
• step_dummy()
• step_zv()
• step_normalize()

── Model ───────────────────────────────────────────────────────────────────────
Linear Regression Model Specification (regression)

Main Arguments:
  penalty = 910.298177991523
  mixture = 0

Computational engine: glmnet </code></pre>
</div>
</div>
<ol start="8" type="1">
<li>Fit the final model and validate the performance.</li>
</ol>
<div class="cell">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ridge_final_fit</span> <span class="op">&lt;-</span> <span class="fu">fit</span><span class="op">(</span><span class="va">ridge_final</span>, data <span class="op">=</span> <span class="va">Hitters_train</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">augment</span><span class="op">(</span><span class="va">ridge_final_fit</span>, new_data <span class="op">=</span> <span class="va">Hitters_test</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">rsq</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">Salary</span>, estimate <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 rsq     standard       0.514</code></pre>
</div>
</div>
<p>To perform a Lasso regression we don’t need to repeat all the code.</p>
<div class="cell">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lasso_spec</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">linear_reg</span><span class="op">(</span>penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>, mixture <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lasso_workflow</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">ridge_recipe</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_model</span><span class="op">(</span><span class="va">lasso_spec</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lasso_penalty_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">penalty</span><span class="op">(</span>range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lasso_tune_res</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">tune_grid</span><span class="op">(</span><span class="va">lasso_workflow</span>,</span>
<span>            resamples <span class="op">=</span> <span class="va">Hitters_fold</span>, </span>
<span>            grid <span class="op">=</span> <span class="va">lasso_penalty_grid</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">lasso_tune_res</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">scale_x_log10</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org/reference/comma.html">comma_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">theme_light</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="05-generalized-linear-models_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section></section><section id="dimension-reduction" class="level2" data-number="5.5"><h2 data-number="5.5" class="anchored" data-anchor-id="dimension-reduction">
<span class="header-section-number">5.5</span> Dimension Reduction</h2>
<p>This method projects the <span class="math inline">\(p\)</span> predictors into an <span class="math inline">\(M\)</span>-dimensional subspace. If <span class="math inline">\(Z_1, Z_2, \dots, Z_M\)</span> represent <span class="math inline">\(M &lt; p\)</span> lineal combinations <span class="math inline">\(Z_m = \sum_{j=1}^p \phi_{jm}X_j\)</span> of <strong>ALL our original predictors</strong> based on some constants <span class="math inline">\(\phi_{1m}, \phi_{2m}, \dots, \phi_{pm}\)</span>, then we can use the new variables to fit a linear regression model by least squares.</p>
<p><span class="math display">\[
y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i,
\qquad i=1, \dots, n.
\]</span></p>
<p>This is not a feature selection method as each of the <em>M</em> principal components used in the regression is a linear combination of all <em>p</em> of the original features.In this sense, PCR is more closely related to <em>ridge regression</em> than to the <em>lasso</em>.</p>
<p>To select the <span class="math inline">\(\phi_{jm}\)</span>’s we will discuss two different ways:</p>
<section id="principal-components-regression-pcr" class="level3" data-number="5.5.1"><h3 data-number="5.5.1" class="anchored" data-anchor-id="principal-components-regression-pcr">
<span class="header-section-number">5.5.1</span> Principal Components Regression (PCR)</h3>
<p>The PCR assumes that <em>the directions in which</em> <span class="math inline">\(X_1, \dots, X_M\)</span> <em>show the most variation are the directions that are associated with</em> <span class="math inline">\(Y\)</span>. If it’s that is true then fitting the model to <span class="math inline">\(Z_1, \dots, Z_m\)</span> will lead better results than using the original variables <span class="math inline">\(X_1, \dots, X_p\)</span></p>
<p>To perform a <em>principal components analysis</em> (PCA):</p>
<ol type="1">
<li>It’s recommended to standardize each predictor to have the same scale.</li>
</ol>
<p><span class="math display">\[
\tilde{x}_{ij} =
\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}} =
\frac{x_{ij}}{\sigma_j}
\]</span></p>
<ol start="2" type="1">
<li>Select as the <em>ﬁrst principal component</em> the variable where the data <em>vary the most</em>.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/29-pca-data-example.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
<ol start="3" type="1">
<li>Project the observations on the first component, to get the largest possible variance</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/30-pca-projection.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
<ol start="4" type="1">
<li><p>Then maximize the <span class="math inline">\(\text{Var}(\phi_{11} \times (x_1-\overline{x_1}) + \phi_{21} \times (x_2-\overline{x_2}))\)</span> where <span class="math inline">\(\phi_{11}^2 + \phi_{21}^2 = 1\)</span> to the get <strong>the principal component loadings</strong>. As result <span class="math inline">\(Z_1\)</span> it’s a weighted average of the to variables.</p></li>
<li><p>Repeat the process until having <em>p</em> distinct principal components <em>perpendicular</em> to the previews one.</p></li>
</ol>
<p>In general, this method performs better when we just need to use <strong>few principal components</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/31-pca-test-error.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/32-pca-test-error-less-components.png" style="width:90.0%;height:90.0%" class="figure-img"></p>
</figure>
</div>
<p>The <strong>number of principal components</strong>, M, is typically chosen by <strong>cross-validation</strong>.</p>
</section><section id="partial-least-squares-pls" class="level3" data-number="5.5.2"><h3 data-number="5.5.2" class="anchored" data-anchor-id="partial-least-squares-pls">
<span class="header-section-number">5.5.2</span> Partial Least Squares (PLS)</h3>
<p>The PLS approach attempts to ﬁnd directions that help explain both the response and the predictors by placing the <strong>highest weight</strong> on the variables that are most <strong>strongly related to the response</strong>.</p>
<ol type="1">
<li>Standardize the predictors and <strong>response</strong>.</li>
</ol>
<p><span class="math display">\[
\tilde{x}_{ij} =
\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2}} =
\frac{x_{ij}}{\sigma_j}
\]</span></p>
<ol start="2" type="1">
<li><p>Compute the ﬁrst direction <span class="math inline">\(Z_1\)</span> by setting each <span class="math inline">\(\phi_{j1}\)</span> equal to the coeﬃcient from the simple linear regression of <span class="math inline">\(Y \sim X_j\)</span>, which it’s also proportional to the correlation.</p></li>
<li><p>Adjust each of the variables for <span class="math inline">\(Z_1\)</span>, by regressing each variable on <span class="math inline">\(Z_1\)</span> and taking residuals.</p></li>
<li><p>Compute <span class="math inline">\(Z_2\)</span> using this <em>orthogonalized data</em> (residuals) in exactly the same fashion as <span class="math inline">\(Z_1\)</span> was computed based on the original data and repete the process <span class="math inline">\(M\)</span> times.</p></li>
<li><p>Use least squares to ﬁt a linear model to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(Z_1, \dots, Z_M\)</span></p></li>
</ol>
<p>To select the number <span class="math inline">\(M\)</span> we can use <strong>cross-validation</strong>.</p>
</section><section id="pcr-vs-pls" class="level3" data-number="5.5.3"><h3 data-number="5.5.3" class="anchored" data-anchor-id="pcr-vs-pls">
<span class="header-section-number">5.5.3</span> PCR vs PLS</h3>
<p>The next figure illustrates how each method work.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/67-PCR-vs-PLS-diagrams.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The next figure illustrates that the first two PCs when using - <strong>PCR</strong>: Has very <strong>little relationship</strong> to the response variable. - <strong>PLS</strong>: Has a much <strong>stronger association</strong> to the response variable.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/68-PCR-vs-PLS-example.png" class="img-fluid figure-img"></p>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Performance Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In practice, it often performs no better than ridge regression or PCR. While the supervised dimension reduction of PLS can reduce bias, it also has the potential to <strong>increase variance</strong>, so that the overall beneﬁt of PLS relative to PCR <strong>is a wash</strong>.</p>
</div>
</div>
</section><section id="coding-example-2" class="level3" data-number="5.5.4"><h3 data-number="5.5.4" class="anchored" data-anchor-id="coding-example-2">
<span class="header-section-number">5.5.4</span> Coding example</h3>
<p>To run a <strong>PCR</strong> or a <strong>PLS</strong> we just need to set a lineal model.</p>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lm_spec</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">linear_reg</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"lm"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And tune our recipe to determinate the <code>threshold</code> and the number of components to use <code>num_comp</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pca_recipe</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">recipe</span><span class="op">(</span>formula <span class="op">=</span> <span class="va">Salary</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Hitters_train</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_novel</span><span class="op">(</span><span class="fu">all_nominal_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_dummy</span><span class="op">(</span><span class="fu">all_nominal_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_zv</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">step_pca</span><span class="op">(</span><span class="fu">all_predictors</span><span class="op">(</span><span class="op">)</span>, </span>
<span>           threshold <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span>,</span>
<span>           num_comp <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pca_workflow</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">pca_recipe</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">add_model</span><span class="op">(</span><span class="va">lm_spec</span><span class="op">)</span></span>
<span></span>
<span><span class="va">threshold_grid</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">threshold</span><span class="op">(</span><span class="op">)</span>, </span>
<span>               <span class="fu">num_comp</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">20</span><span class="op">)</span><span class="op">)</span>,</span>
<span>               levels <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pca_tune_res</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu">tune_grid</span><span class="op">(</span><span class="va">pca_workflow</span>,</span>
<span>            resamples <span class="op">=</span> <span class="va">Hitters_fold</span>, </span>
<span>            grid <span class="op">=</span> <span class="va">threshold_grid</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As we can see below the number of component don’t have any effect over the test error.</p>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">pca_tune_res</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">scale_x_log10</span><span class="op">(</span>labels <span class="op">=</span> <span class="fu">scales</span><span class="fu">::</span><span class="fu"><a href="https://scales.r-lib.org/reference/comma.html">comma_format</a></span><span class="op">(</span>accuracy <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">theme_light</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="05-generalized-linear-models_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">best_threshold</span> <span class="op">&lt;-</span> <span class="fu">select_best</span><span class="op">(</span><span class="va">pca_tune_res</span>, metric <span class="op">=</span> <span class="st">"rmse"</span><span class="op">)</span></span>
<span><span class="va">best_threshold</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  num_comp threshold .config              
     &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;                
1        1      0.75 Preprocessor04_Model1</code></pre>
</div>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">final_pcr_fit</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">pca_workflow</span>, <span class="va">best_threshold</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="va">Hitters_train</span><span class="op">)</span></span>
<span></span>
<span><span class="va">final_pcr_fit</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">augment</span><span class="op">(</span>new_data <span class="op">=</span> <span class="va">Hitters_test</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">rmse</span><span class="op">(</span>truth <span class="op">=</span> <span class="va">Salary</span>, estimate <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric .estimator .estimate
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
1 rmse    standard        402.</code></pre>
</div>
</div>
<p>But thanks to the <code>DALEXtra</code> package we can interpret these complex models.</p>
<div class="cell">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ModelOriented.github.io/DALEXtra/">DALEXtra</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1807</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ModelOriented.github.io/DALEXtra/reference/explain_tidymodels.html">explain_tidymodels</a></span><span class="op">(</span></span>
<span>  <span class="va">final_pcr_fit</span>, </span>
<span>  data <span class="op">=</span> <span class="va">Hitters_train</span> <span class="op">%&gt;%</span> <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">Salary</span><span class="op">)</span>, </span>
<span>  y <span class="op">=</span> <span class="va">Hitters_train</span><span class="op">$</span><span class="va">Salary</span>,</span>
<span>  label <span class="op">=</span> <span class="st">"RDA"</span>,</span>
<span>  verbose <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu"><a href="https://modeloriented.github.io/DALEX/reference/model_parts.html">model_parts</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="05-generalized-linear-models_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section></section><section id="poisson-regression" class="level2" data-number="5.6"><h2 data-number="5.6" class="anchored" data-anchor-id="poisson-regression">
<span class="header-section-number">5.6</span> Poisson Regression</h2>
<p>If match with the next conditions we can fit a model based on <strong>Poisson Distribution</strong>:</p>
<ul>
<li>The outcome variable is the result of <strong>counting</strong>, so <span class="math inline">\(Y \in \{ 0, 1, 2, 3, \dots \}\)</span>.</li>
<li>$Ave(Y) Var(Y) $, if not you can use the <strong>Quasipoisson Distribution</strong>.</li>
</ul>
<p>The <em>Poisson Distribution</em> follow the next function:</p>
<p><span class="math display">\[
Pr(Y = k) = \frac{e^{-\lambda} \lambda^{k}}
                 {k!}
\]</span></p>
<p>Where: - <span class="math inline">\(\lambda\)</span> must be greater than 0. It represents the expected number of events <span class="math inline">\(E(Y)\)</span> and variance related <span class="math inline">\(Var(Y)\)</span> - <span class="math inline">\(k\)</span> represent the number of events that we want to evaluate base of <span class="math inline">\(\lambda\)</span>. Its numbers should be greater or equal to 0.</p>
<p>So, it makes sense that the value that we want to predict with our regression would be <span class="math inline">\(\lambda\)</span>, by using next structure:</p>
<p><span class="math display">\[
\log{ \left( \lambda(X_1, X_2, \dots , X_p)  \right)} = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
\]</span></p>
<p>If select this model we need to be aware how to interpret the coefficients. For example, if <span class="math inline">\(\beta_1 = -0.08\)</span> for a categorical variable, we can conclude by calculating <span class="math inline">\(e^{-0.08}\)</span> that <strong><em>92.31%</em></strong> of events of the base line related to <span class="math inline">\(\beta_0\)</span> would happen.</p>
<section id="coding-example-3" class="level3" data-number="5.6.1"><h3 data-number="5.6.1" class="anchored" data-anchor-id="coding-example-3">
<span class="header-section-number">5.6.1</span> Coding example</h3>
<p>To perform <strong>Poisson Regression</strong> we just need to create the model specification by loading the <strong>poissonreg</strong> package and using <strong>glm</strong> engine.</p>
<div class="cell">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidymodels/poissonreg">poissonreg</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">pois_spec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://parsnip.tidymodels.org/reference/poisson_reg.html">poisson_reg</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://parsnip.tidymodels.org/reference/set_args.html">set_mode</a></span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://parsnip.tidymodels.org/reference/set_engine.html">set_engine</a></span><span class="op">(</span><span class="st">"glm"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">pois_rec_spec</span> <span class="op">&lt;-</span> <span class="va">lm_rec_spec</span></span>
<span></span>
<span><span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">add_model</span><span class="op">(</span><span class="va">pois_spec</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">pois_rec_spec</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">last_fit</span><span class="op">(</span>split <span class="op">=</span> <span class="va">BikeshareSpit</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">collect_metrics</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 4
  .metric .estimator .estimate .config             
  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               
1 rmse    standard      69.7   Preprocessor1_Model1
2 rsq     standard       0.725 Preprocessor1_Model1</code></pre>
</div>
</div>
</section></section><section id="logistic-regression" class="level2" data-number="5.7"><h2 data-number="5.7" class="anchored" data-anchor-id="logistic-regression">
<span class="header-section-number">5.7</span> Logistic Regression</h2>
<p>It models the <strong>probability</strong> (<span class="math inline">\(p(X) = Pr(Y=1|X)\)</span>) that Y belongs to a particular category given some predictors by assuming that <span class="math inline">\(Y\)</span> follows a <strong>Bernoulli Distribution</strong>. This model calculates the probability using the <strong><em>logistic function</em></strong> which produce a S form between 0 and 1:</p>
<p><span class="math display">\[
p(X) = \frac{e^{\beta_{0}+\beta_{1}X}}
            {1+e^{\beta_{0}+\beta_{1}X}}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/09-logistic-function-example.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As the functions returns probabilities is responsibility of the analyst to define a <strong>threshold</strong> to make classifications.</p>
<section id="estimating-coefficients" class="level3" data-number="5.7.1"><h3 data-number="5.7.1" class="anchored" data-anchor-id="estimating-coefficients">
<span class="header-section-number">5.7.1</span> Estimating coefficients</h3>
<p>To estimate <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> the method used is called as <em>maximum likelihood</em> which consists in maximizing the <strong>likelihood function</strong>. It is important to clarify that the <em>least squares approach</em> is in fact a special case of maximum likelihood.</p>
<p><span class="math display">\[
\ell(\beta_{0}, \beta_{1}) = \prod_{i:y_{i} = 1}p(x_{i})\prod_{i':y_{i'} = 0}p(1-x_{i'})
\]</span></p>
</section><section id="multiple-regression" class="level3" data-number="5.7.2"><h3 data-number="5.7.2" class="anchored" data-anchor-id="multiple-regression">
<span class="header-section-number">5.7.2</span> Multiple regression</h3>
<p>We also can generalize the <em>logistic function</em> as you can see bellow.</p>
<p><span class="math display">\[
p(X) = \frac{e^{\beta_{0}+\beta_{1}X_{1}+\dots+\beta_{p}X_{p}}}
            {1+e^{\beta_{0}+\beta_{1}X_{1}+\dots+\beta_{p}X_{p}}}
\]</span></p>
</section><section id="interpreting-the-model" class="level3" data-number="5.7.3"><h3 data-number="5.7.3" class="anchored" data-anchor-id="interpreting-the-model">
<span class="header-section-number">5.7.3</span> Interpreting the model</h3>
<p>To understand how each variable influence the probability <span class="math inline">\(p(X)\)</span>, we need to manipulate the <em>logistic function</em> until having a lineal combination on the right site.</p>
<p><span class="math display">\[
\underbrace{ \log{ \left( \overbrace{\frac{p(X)}{1 - p(X)}}^\text{odds ratio} \right)} }_\text{log odds or logit} = \beta_{0}+\beta_{1}X
\]</span></p>
<p>As we can see, the result of the linear combination is the <span class="math inline">\(\log\)</span> of the <em>odds ratio</em>, known as <strong>log odd</strong> or <strong>logit</strong>.</p>
<p>An <strong>odds ratio</strong> of an event presents the likelihood that the event will occur as a proportion of the likelihood that the event won’t occur. It can take any value between <span class="math inline">\(0\)</span> and <span class="math inline">\(\infty\)</span>, where low probabilities are close to <span class="math inline">\(0\)</span>, higher to <span class="math inline">\(\infty\)</span> and equivalents ones are equals to 1. For example, if we have an <span class="math inline">\(\text{odds ratio} = 2\)</span>, we can say that it’s 2 times more likely that the event happens rather than not.</p>
<p>Applying <span class="math inline">\(\log{(\text{odds ratio})}\)</span> makes easier to compare the effect of variables as values below 1 become negative numbers of the scale of possible numbers and 1 becomes 0 for non-significant ones. To have an idea, an odds ratio of 2 has the same effect as 0.5, which it’s hard to see at first hand, but if we apply the <span class="math inline">\(\log\)</span> to each value we can see that <span class="math inline">\(\log{(2)} = 0.69\)</span> and <span class="math inline">\(\log{(0.5)} = -0.69\)</span>.</p>
<p>At end, <span class="math inline">\(p(X)\)</span> will increase as <span class="math inline">\(X\)</span> increases if <span class="math inline">\(\beta_{1}\)</span> is positive despite the relationship between each other isn’t a linear one.</p>
<section id="understanding-a-confounding-paradox" class="level4"><h4 class="anchored" data-anchor-id="understanding-a-confounding-paradox">Understanding a confounding paradox</h4>
<table class="table">
<colgroup>
<col style="width: 44%">
<col style="width: 55%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Simple Regression</th>
<th style="text-align: left;">Multiple Regression</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><img src="img/10-Default-table-4_2.png" class="img-fluid" data-fig-align="center"></td>
<td style="text-align: left;"><img src="img/11-Default-table-4_3.png" class="img-fluid" data-fig-align="center"></td>
</tr>
<tr class="even">
<td style="text-align: left;">The positive coeﬃcient for student indicates that for <strong>over all values of balance and income</strong>, a student is <em>more</em> likely to default than a non-student.</td>
<td style="text-align: left;">The negative coeﬃcient for student indicates that for a <strong>ﬁxed value of balance and income</strong>, a student is less likely to default than a non-student.</td>
</tr>
</tbody>
</table>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/12-Default-multiple-plot-4_3.png" style="width:60.0%;height:60.0%" class="figure-img"></p>
</figure>
</div>
<p>The problem relays on the fact that <em>student</em> and <em>balance</em> are <strong>correlated</strong>. In consequence, a student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the <em>same credit card balance</em>!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/13-Default-simple-plot-4_3.png" style="width:60.0%;height:60.0%" class="figure-img"></p>
</figure>
</div>
</section><section id="multinomial-logistic-regression" class="level4"><h4 class="anchored" data-anchor-id="multinomial-logistic-regression">Multinomial Logistic Regression</h4>
<p>We also can generalize the <em>logistic function</em> to support more than 2 categories (<span class="math inline">\(K &gt; 2\)</span>) by defining by convention the last category <span class="math inline">\(K\)</span> as a <strong>baseline</strong>.</p>
<p>For <span class="math inline">\(k = 1, \dotsc,K-1\)</span> we use function.</p>
<p><span class="math display">\[
Pr(Y = k|X= x) = \frac{e^{\beta_{k0}+\beta_{k1}x_{1}+\dots+\beta_{kp}x_{p}}}
                      {1+\sum_{l=1}^{K-1}e^{\beta_{l0}+\beta_{l1}x_{1}+\dots+\beta_{lp}x_{p}}}
\]</span></p>
<p>For <span class="math inline">\(k=K\)</span>, we use the function.</p>
<p><span class="math display">\[
Pr(Y = K|X= x) = \frac{1}
                      {1+\sum_{l=1}^{K-1}e^{\beta_{l0}+\beta_{l1}x_{1}+\dots+\beta_{lp}x_{p}}}
\]</span> And after some manipulations we can show that <span class="math inline">\(\log\)</span> of the probability of getting <span class="math inline">\(k\)</span> divided by the probability of the <em>baseline</em> is equivalent to a linear combinations of the functions parameters.</p>
<p><span class="math display">\[
\log{ \left( \frac{Pr(Y = k|X= x)}{Pr(Y = K|X= x)} \right)} = \beta_{k0}+\beta_{k1}x_{1}+\dots+\beta_{kp}x_{p}
\]</span></p>
<p>In consequence, each coefficient represent a measure of how much change the probability from the baseline probability.</p>
</section></section><section id="model-limitatios" class="level3" data-number="5.7.4"><h3 data-number="5.7.4" class="anchored" data-anchor-id="model-limitatios">
<span class="header-section-number">5.7.4</span> Model limitatios</h3>
<p>There are models that could make better classifications when:</p>
<ul>
<li>There is a substantial separation between the <span class="math inline">\(Y\)</span> classes.</li>
<li>The predictors <span class="math inline">\(X\)</span> are approximately normal in each class and the sample size is small.</li>
<li>When the decision boundary is not lineal.</li>
</ul></section><section id="coding-example-4" class="level3" data-number="5.7.5"><h3 data-number="5.7.5" class="anchored" data-anchor-id="coding-example-4">
<span class="header-section-number">5.7.5</span> Coding example</h3>
<p>To perform <strong>Logistic Regression</strong> we just need to create the model specification by loading the <strong>discrim</strong> package and using <strong>MASS</strong> engine.</p>
<div class="cell">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">Smarket_train</span> <span class="op">&lt;-</span> </span>
<span>  <span class="va">Smarket</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">!=</span> <span class="fl">2005</span><span class="op">)</span></span>
<span></span>
<span><span class="va">Smarket_test</span> <span class="op">&lt;-</span> </span>
<span>  <span class="va">Smarket</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">==</span> <span class="fl">2005</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lr_spec</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://parsnip.tidymodels.org/reference/logistic_reg.html">logistic_reg</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://parsnip.tidymodels.org/reference/set_engine.html">set_engine</a></span><span class="op">(</span><span class="st">"glm"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://parsnip.tidymodels.org/reference/set_args.html">set_mode</a></span><span class="op">(</span><span class="st">"classification"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">SmarketLrPredictions</span> <span class="op">&lt;-</span></span>
<span>  <span class="va">lr_spec</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, data <span class="op">=</span> <span class="va">Smarket_train</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span>new_data <span class="op">=</span> <span class="va">Smarket_test</span><span class="op">)</span> </span>
<span></span>
<span></span>
<span><span class="fu">conf_mat</span><span class="op">(</span><span class="va">SmarketLrPredictions</span>, truth <span class="op">=</span> <span class="va">Direction</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Truth
Prediction Down  Up
      Down   35  35
      Up     76 106</code></pre>
</div>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">accuracy</span><span class="op">(</span><span class="va">SmarketLrPredictions</span>, truth <span class="op">=</span> <span class="va">Direction</span>, estimate <span class="op">=</span> <span class="va">.pred_class</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy binary         0.560</code></pre>
</div>
</div>
</section></section><section id="generalized-additive-models-gam" class="level2" data-number="5.8"><h2 data-number="5.8" class="anchored" data-anchor-id="generalized-additive-models-gam">
<span class="header-section-number">5.8</span> Generalized Additive Models (GAM)</h2>
<p>They provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity.</p>
<p>To transform each predictor we have the next options:</p>
<ul>
<li>A constant for each categorical level (step function)</li>
<li>Polynomial regression</li>
<li>Natural spines (optimized with least squares)</li>
<li>Smoothing splines (optimized with backﬁtting)</li>
<li>Local regression</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Backﬁtting</strong> ﬁts a model involving multiple predictors by repeatedly updating the ﬁt for each predictor in turn, holding the others ﬁxed.</p>
</div>
</div>
</div>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Pros</th>
<th style="text-align: left;">Cons</th>
</tr></thead>
<tbody><tr class="odd">
<td style="text-align: left;">1. It <strong>finds relationships that a lineal model would miss</strong> without applying many transformations as it can fit a non-linear <span class="math inline">\(f_j\)</span> to each <span class="math inline">\(X_j\)</span> <br><br> 2. We can <strong>examine the eﬀect of each predictor on <span class="math inline">\(Y\)</span></strong> individually as the model is additive. <br><br> 3. The <em>smoothness</em> of each function <span class="math inline">\(f_j\)</span> can be summarized via <strong>degrees of freedom</strong>.</td>
<td style="text-align: left;">1. Important <strong>interactions can be missed</strong>, but they can be added manually</td>
</tr></tbody>
</table>
<section id="gam-regression" class="level3" data-number="5.8.1"><h3 data-number="5.8.1" class="anchored" data-anchor-id="gam-regression">
<span class="header-section-number">5.8.1</span> GAM Regression</h3>
<p>To predict a numeric variable this method creates a function with the next form:</p>
<p><span class="math display">\[
y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i
\]</span></p>
<p>By taking the other predictor as constant we can plot effect of each function for each predictor in the <code>Wage</code> example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/43-gam-regression.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="gam-classification" class="level3" data-number="5.8.2"><h3 data-number="5.8.2" class="anchored" data-anchor-id="gam-classification">
<span class="header-section-number">5.8.2</span> GAM Classification</h3>
<p>To predict a categorical variable this method creates a function with the next form:</p>
<p><span class="math display">\[
\log \left( \frac{p(X)}{1-p(X)} \right)= \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \dots + f_p(x_{ip})
\]</span></p>
<p>By taking the other predictor as constant we can plot effect of each function for each predictor in the <code>Wage</code> example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/44-gam-classification.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section><section id="coding-example-5" class="level3" data-number="5.8.3"><h3 data-number="5.8.3" class="anchored" data-anchor-id="coding-example-5">
<span class="header-section-number">5.8.3</span> Coding example</h3>
<p>The <code>mgcv</code> package supports the next families based on the outcome variable type.</p>
<ul>
<li>
<strong>Gaussian (Default):</strong> Regular regression</li>
<li>
<strong>Binomial:</strong> Probabilities</li>
<li>
<strong>Poisson/Quasipoisson:</strong> counts</li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidymodels.tidymodels.org">tidymodels</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">mgcv</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/load.html">load</a></span><span class="op">(</span><span class="st">"data/Soybean.RData"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">gam_fitted</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://parsnip.tidymodels.org/reference/gen_additive_mod.html">gen_additive_mod</a></span><span class="op">(</span>mode <span class="op">=</span> <span class="st">"regression"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://parsnip.tidymodels.org/reference/set_engine.html">set_engine</a></span><span class="op">(</span>engine <span class="op">=</span> <span class="st">"mgcv"</span>,</span>
<span>             family <span class="op">=</span> <span class="va">gaussian</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html">fit</a></span><span class="op">(</span><span class="va">weight</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/pkg/mgcv/man/s.html">s</a></span><span class="op">(</span><span class="va">Time</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">soybean_train</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">gam_fitted</span>, </span>
<span>        new_data <span class="op">=</span> <span class="va">soybean_train</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">Time</span>, <span class="va">weight</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>y <span class="op">=</span> <span class="va">.pred</span><span class="op">)</span>,</span>
<span>            color <span class="op">=</span> <span class="st">"blue"</span>,</span>
<span>            linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">theme_light</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="05-generalized-linear-models_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://hardhat.tidymodels.org/reference/hardhat-extract.html">extract_fit_engine</a></span><span class="op">(</span><span class="va">gam_fitted</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Family: gaussian 
Link function: identity 

Formula:
weight ~ s(Time)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   6.1645     0.1143   53.93   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Approximate significance of smooth terms:
          edf Ref.df     F p-value    
s(Time) 8.495   8.93 338.2  &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

R-sq.(adj) =  0.902   Deviance explained = 90.4%
GCV = 4.4395  Scale est. = 4.3117    n = 330</code></pre>
</div>
</div>


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./04-resampling-methods.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Resampling methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-generative-classification-models.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Generative Models for Classiﬁcation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>


</body></html>